{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfZf7/yyOXu9Bq6+stzcJn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yezzzzin/object-detectinon/blob/main/data_preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy.io.wavfile as wav\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Bidirectional, Dense, TimeDistributed\n",
        "\n",
        "def compute_mfcc(audio_signal, sample_rate=44100, frame_size=0.025, frame_stride=0.01, num_mfcc=13, nfilt=26, nfft=512):\n",
        "    # Step 1: Frame Segmentation\n",
        "    frame_length, frame_step = frame_size * sample_rate, frame_stride * sample_rate\n",
        "    signal_length = len(audio_signal)\n",
        "    frame_length = int(round(frame_length))\n",
        "    frame_step = int(round(frame_step))\n",
        "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
        "\n",
        "    # zero padding\n",
        "    pad_signal_length = num_frames * frame_step + frame_length\n",
        "    z = np.zeros((pad_signal_length - signal_length))\n",
        "    pad_signal = np.append(audio_signal, z)\n",
        "\n",
        "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
        "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
        "\n",
        "    # MFCC 계산\n",
        "    mfccs = []\n",
        "    for frame in frames:\n",
        "        # Apply pre-emphasis filter\n",
        "        emphasized_signal = np.append(frame[0], frame[1:] - 0.97 * frame[:-1])\n",
        "\n",
        "        # Apply Hamming window\n",
        "        windowed_signal = emphasized_signal * np.hamming(len(emphasized_signal))\n",
        "\n",
        "        # Compute FFT\n",
        "        magnitude_spectrum = np.abs(fft(windowed_signal, nfft)[:nfft//2])\n",
        "\n",
        "        # Compute power spectrum\n",
        "        power_spectrum = (1.0 / nfft) * (magnitude_spectrum ** 2)\n",
        "\n",
        "        # Apply Mel filterbank\n",
        "        mel_filters = get_mel_filterbank(nfilt, nfft, sample_rate)\n",
        "        mel_spectrum = np.dot(power_spectrum, mel_filters.T)\n",
        "\n",
        "        # Take logarithm\n",
        "        log_mel_spectrum = np.log(mel_spectrum + 1e-10)\n",
        "\n",
        "        # Apply DCT to get MFCC coefficients\n",
        "        mfcc = dct(log_mel_spectrum, type=2, axis=1, norm='ortho')[:, 1 : (num_mfcc + 1)]\n",
        "        mfccs.append(mfcc)\n",
        "\n",
        "    return np.array(mfccs)\n",
        "\n",
        "def get_mel_filterbank(nfilt=26, nfft=512, sample_rate=44100, low_freq=0, high_freq=None):\n",
        "    high_freq = high_freq or sample_rate // 2\n",
        "    mel_points = np.linspace(hz_to_mel(low_freq), hz_to_mel(high_freq), nfilt + 2)\n",
        "    hz_points = mel_to_hz(mel_points)\n",
        "    bin_points = np.floor((nfft + 1) * hz_points / sample_rate).astype(int)\n",
        "    filterbank = np.zeros((nfilt, nfft // 2 + 1))\n",
        "\n",
        "    for i in range(1, nfilt + 1):\n",
        "        filterbank[i - 1, bin_points[i - 1] : bin_points[i]] = (bin_points[i] - bin_points[i - 1]) / (bin_points[i + 1] - bin_points[i])\n",
        "        filterbank[i - 1, bin_points[i] : bin_points[i + 1]] = (bin_points[i + 1] - bin_points[i]) / (bin_points[i + 1] - bin_points[i])\n",
        "\n",
        "    return filterbank\n",
        "\n",
        "def hz_to_mel(hz):\n",
        "    return 2595 * np.log10(1 + hz / 700)\n",
        "\n",
        "def mel_to_hz(mel):\n",
        "    return 700 * (10**(mel / 2595) - 1)\n",
        "\n",
        "# 여러 개의 WAV 파일 읽기\n",
        "file_paths = [\"audio1.wav\", \"audio2.wav\", \"audio3.wav\"]\n",
        "\n",
        "mfccs_list = []  # 각 WAV 파일의 MFCC를 저장할 리스트\n",
        "\n",
        "for file_path in file_paths:\n",
        "    sample_rate, audio_signal = wav.read(file_path)\n",
        "    mfccs = compute_mfcc(audio_signal, sample_rate)\n",
        "    mfccs_list.append(mfccs)\n",
        "\n",
        "# MFCCs를 numpy 배열로 변환하여 BiLSTM에 입력\n",
        "X_train = np.array(mfccs_list)\n",
        "\n",
        "# BiLSTM 모델 정의\n",
        "model = Sequential()\n",
        "model.add(Bidirectional(LSTM(units=64, return_sequences=True), input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(TimeDistributed(Dense(1, activation='sigmoid')))\n",
        "\n",
        "# 모델 컴파일 및 학습\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n"
      ],
      "metadata": {
        "id": "PuFas5GZaW8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}